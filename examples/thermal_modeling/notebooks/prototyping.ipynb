{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c292df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Revise \n",
    "#includet(\"../src/thermal_modeling.jl\")\n",
    "using Flux, DataFrames, CSV, ProgressMeter, Statistics\n",
    "#using thermal_modeling: TNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd3e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topology Definitions\n",
    "mutable struct HeatTransferLayer{U,V,T}\n",
    "    n_temps::Int\n",
    "    n_targets::Int\n",
    "    conductance_net::Dense{U,Matrix{V},Vector{T}}\n",
    "    adj_mat::Matrix{Int8}\n",
    "end\n",
    "\n",
    "function HeatTransferLayer(n_input::Integer, n_temps::Integer, n_targets::Integer)\n",
    "    # populate adjacency matrix\n",
    "    adj_mat = zeros(Int8, n_temps, n_temps)\n",
    "    k = 1\n",
    "    for col_j in 1:n_temps\n",
    "        for row_i in col_j + 1:n_temps\n",
    "            adj_mat[row_i, col_j] = k\n",
    "            k += 1\n",
    "        end\n",
    "    end\n",
    "    adj_mat = adj_mat + adj_mat'\n",
    "    n_conds = Int(0.5 * n_temps * (n_temps - 1))\n",
    "    HeatTransferLayer(n_temps, n_targets,\n",
    "                      Dense(n_input + n_targets, n_conds, σ),\n",
    "                      adj_mat)\n",
    "end\n",
    "\n",
    "# overload struct to make it callable\n",
    "function (m::HeatTransferLayer)(all_input)\n",
    "    n_temps = m.n_temps\n",
    "    prev_out = @view all_input[1:m.n_targets, :]\n",
    "    temps = @view all_input[1:n_temps, :]\n",
    "    \n",
    "    conductances = m.conductance_net(all_input)\n",
    "    \n",
    "    # subtract, scale, and sum\n",
    "    tmp = hcat([sum(temps[j, :] .- prev_out[i, :] .* conductances[m.adj_mat[i, j], :] \n",
    "                for j in 1:n_temps if j != i) \n",
    "                    for i in 1:m.n_targets]...)'\n",
    "    # mutating arrays not allowed in zygote\n",
    "    \"\"\"tmp = zeros(eltype(prev_out), size(prev_out))\n",
    "    for i in 1:m.n_targets\n",
    "        for j in 1:n_temps\n",
    "            if j != i\n",
    "                @. tmp[i, :] += (temps[j, :] - prev_out[i, :]) * conductances[m.adj_mat[i, j], :]\n",
    "            end\n",
    "        end\n",
    "    end\"\"\"\n",
    "\n",
    "\n",
    "    return tmp\n",
    "end\n",
    "\n",
    "# specify what is trainable \n",
    "Flux.@functor HeatTransferLayer (conductance_net,)\n",
    "\n",
    "mutable struct TNNCell{U <: Chain,V <: Real,S}\n",
    "    sample_time::V\n",
    "    ploss_net::U\n",
    "    heat_net::HeatTransferLayer\n",
    "    caps::Vector{V}\n",
    "    prll::Parallel  # will be defined in inner constructor (no outer definition)\n",
    "    state0::S\n",
    "    function TNNCell(sample_time::V, ploss_net::U, heat_net::HeatTransferLayer, caps::Vector{V}, init_hidden::S) where {U <: Chain,V <: Real,S}\n",
    "        new{U,V,S}(sample_time, ploss_net, heat_net, caps, Parallel(+, ploss_net, heat_net), init_hidden)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function TNNCell(n_input::U, n_temps::U, n_targets::U, init_hidden::S) where {U <: Integer,S}\n",
    "    ploss_net = Chain(Dense(n_input + n_targets, 8, σ),\n",
    "                      Dense(8, n_targets, σ))\n",
    "    heat_transfer = HeatTransferLayer(n_input, n_temps, n_targets)\n",
    "    caps = 0.5f0 .* randn(Float32, n_targets) .- 3f0  # Gaussian mean=-3 std=0.5\n",
    "    TNNCell(Float32(0.5), ploss_net, heat_transfer, caps, init_hidden)\n",
    "end\n",
    "\n",
    "function (m::TNNCell)(prev_̂y, x)\n",
    "    x_non_temps, x_temps = x\n",
    "    xx = vcat(prev_̂y, x_temps, x_non_temps)\n",
    "    rh_ode = m.prll(xx)\n",
    "    y = prev_̂y .+ m.sample_time .* 10f0.^m.caps .* rh_ode\n",
    "    return y, prev_̂y\n",
    "end\n",
    "\n",
    "# specify what is trainable \n",
    "Flux.@functor TNNCell (ploss_net, heat_net, caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1012ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "const n_input_temps = 2\n",
    "const n_input_non_temps = 3\n",
    "const n_total_inputs = n_input_temps + n_input_non_temps\n",
    "const n_targets = 3\n",
    "const n_temps = n_targets + n_input_temps\n",
    "const n_profiles = 49\n",
    "\n",
    "# smoke-test the topology\n",
    "xs = [(rand(Float32, n_input_non_temps, n_profiles), \n",
    "        rand(Float32, n_input_temps, n_profiles)) for i in 1:10]\n",
    "h = rand(Float32, n_targets, n_profiles)  # initial hidden state\n",
    "\n",
    "m = Flux.Recur(TNNCell(n_input_non_temps+n_input_temps, n_temps, n_targets, h), h)\n",
    "\n",
    "# predict\n",
    "ys = [m(x) for x in xs]\n",
    "ys[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60596e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ys = [rand(Float32, n_targets, n_profiles) for i in 1:10]\n",
    "loss(x_l, y_l) = Statistics::mean(Flux.Losses.mse(m(x), y) for (x, y) in zip(x_l, y_l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0792c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function load_dataset(path::String)::DataFrame\n",
    "    data = CSV.File(path) |> DataFrame\n",
    "    # FE\n",
    "    @. data[!, :i_norm] = sqrt(data.i_d^2 + data.i_q^2)\n",
    "    data[!, :fe1] =\n",
    "        data.i_norm / maximum(data.i_norm) .* data.motor_speed / maximum(data.motor_speed)\n",
    "    data\n",
    "end;\n",
    "\n",
    "mutable struct TemperatureDataSet\n",
    "    train_tnsr::Tuple{Any, Vararg{Any}}\n",
    "    val_tnsr::Tuple{Any, Vararg{Any}}\n",
    "    test_tnsr::Tuple{Any, Vararg{Any}}\n",
    "    non_temp_cols::Vector{String}\n",
    "    temp_cols::Vector{String}\n",
    "    inp_temp_cols::Vector{String}\n",
    "    target_cols::Vector{String}\n",
    "\n",
    "    function TemperatureDataSet(\n",
    "        data_df::DataFrame,\n",
    "        p_id::String,\n",
    "        test_ids::Vector{Int},\n",
    "        val_ids::Vector{Int},\n",
    "        target_cols::Vector{String},\n",
    "        tbptt_len::Int,\n",
    "    )\n",
    "        gdf = groupby(data_df, p_id)\n",
    "        p_sizes = combine(gdf, nrow)\n",
    "        max_len_test = maximum(filter(:profile_id => n -> n in test_set_pids, p_sizes).nrow)\n",
    "        max_len_train = maximum(filter(:profile_id => n -> n ∉ test_set_pids, p_sizes).nrow)\n",
    "\n",
    "        n_test_profiles = length(test_set_pids)\n",
    "        n_train_profiles = length(keys(gdf)) - n_test_profiles\n",
    "\n",
    "        c_input_temps = [\"ambient\", \"coolant\"]\n",
    "        c_non_temps = [\n",
    "            c for\n",
    "            c in names(data_df) if c ∉ [target_cols..., c_input_temps..., \"profile_id\"]\n",
    "        ]\n",
    "\n",
    "        # create placeholders\n",
    "        train_tensor_non_temp_x =\n",
    "            zeros(Float32, (max_len_train, length(c_non_temps), n_train_profiles))\n",
    "        train_tensor_temps_x =\n",
    "            zeros(Float32, (max_len_train, length(c_input_temps), n_train_profiles))\n",
    "        train_tensor_y =\n",
    "            zeros(Float32, (max_len_train, length(target_cols), n_train_profiles))\n",
    "        train_sample_weights = zeros(Float32, (max_len_train, n_train_profiles))\n",
    "\n",
    "        test_tensor_non_temp_x =\n",
    "            zeros(Float32, (max_len_test, length(c_non_temps), n_test_profiles))\n",
    "        test_tensor_temps_x =\n",
    "            zeros(Float32, (max_len_test, length(c_input_temps), n_test_profiles))\n",
    "        test_tensor_y = zeros(Float32, (max_len_test, length(target_cols), n_test_profiles))\n",
    "        test_sample_weights = zeros(Float32, (max_len_test, n_test_profiles))\n",
    "\n",
    "        # fill placeholders\n",
    "        test_p_idx = 0\n",
    "        train_p_idx = 0\n",
    "        @showprogress 0.5 \"Computing \" for (pid, df) in pairs(gdf)\n",
    "            if pid.profile_id ∈ test_set_pids\n",
    "                test_p_idx += 1\n",
    "                test_tensor_non_temp_x[1:nrow(df), :, test_p_idx] .= df[:, c_non_temps]\n",
    "                test_tensor_temps_x[1:nrow(df), :, test_p_idx] .= df[:, c_input_temps]\n",
    "                test_tensor_y[1:nrow(df), :, test_p_idx] .= df[:, target_cols]\n",
    "                test_sample_weights[1:nrow(df), test_p_idx] .= 1\n",
    "            else\n",
    "                train_p_idx += 1\n",
    "                train_tensor_non_temp_x[1:nrow(df), :, train_p_idx] .= df[:, c_non_temps]\n",
    "                train_tensor_temps_x[1:nrow(df), :, train_p_idx] .= df[:, c_input_temps]\n",
    "                train_tensor_y[1:nrow(df), :, train_p_idx] .= df[:, target_cols]\n",
    "                train_sample_weights[1:nrow(df), train_p_idx] .= 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        tbptt_len = 128\n",
    "\n",
    "        train_vec_temps_x =\n",
    "            [train_tensor_temps_x[i, :, :] for i = 1:size(train_tensor_temps_x, 1)]\n",
    "        train_vec_non_temp_x =\n",
    "            [train_tensor_non_temp_x[i, :, :] for i = 1:size(train_tensor_non_temp_x, 1)]\n",
    "        train_vec_x = collect(zip(train_vec_non_temp_x, train_vec_temps_x))\n",
    "        train_vec_y = [train_tensor_y[i, :, :] for i = 1:size(train_tensor_y, 1)]\n",
    "        train_vec_sample_weights =\n",
    "            [train_sample_weights[i, :] for i = 1:size(train_sample_weights, 1)]\n",
    "\n",
    "        train_vec_chunked_x = []\n",
    "        train_vec_chunked_y = []\n",
    "        train_vec_chunked_w = []\n",
    "\n",
    "        i = 0\n",
    "        while i * tbptt_len <= max_len_train\n",
    "            push!(\n",
    "                train_vec_chunked_x,\n",
    "                train_vec_x[i*tbptt_len+1:minimum((\n",
    "                    (i + 1) * tbptt_len + 1,\n",
    "                    max_len_train,\n",
    "                ))],\n",
    "            )\n",
    "            push!(\n",
    "                train_vec_chunked_y,\n",
    "                train_vec_y[i*tbptt_len+1:minimum((\n",
    "                    (i + 1) * tbptt_len + 1,\n",
    "                    max_len_train,\n",
    "                ))],\n",
    "            )\n",
    "            push!(\n",
    "                train_vec_chunked_w,\n",
    "                train_vec_sample_weights[i*tbptt_len+1:minimum((\n",
    "                    (i + 1) * tbptt_len + 1,\n",
    "                    max_len_train,\n",
    "                ))],\n",
    "            )\n",
    "            i += 1\n",
    "        end\n",
    "\n",
    "        test_vec_temps_x =\n",
    "            [test_tensor_temps_x[i, :, :] for i = 1:size(test_tensor_temps_x, 1)]\n",
    "        test_vec_non_temp_x =\n",
    "            [test_tensor_non_temp_x[i, :, :] for i = 1:size(test_tensor_non_temp_x, 1)]\n",
    "        test_vec_x = collect(zip(train_vec_non_temp_x, test_vec_temps_x))\n",
    "        test_vec_y = [test_tensor_y[i, :, :] for i = 1:size(test_tensor_y, 1)]\n",
    "        test_vec_sample_weights =\n",
    "            [test_sample_weights[i, :] for i = 1:size(test_sample_weights, 1)]\n",
    "\n",
    "        # TODO\n",
    "        # val_vec creation and chunking missing\n",
    "        # test_vec chunking missing\n",
    "        new(\n",
    "            (train_vec_chunked_x, train_vec_chunked_y, train_vec_chunked_w),\n",
    "            (zeros(1), zeros(1), zeros(1)), # not implemented yet\n",
    "            (zeros(1), zeros(1), zeros(1)), # not implemented yet\n",
    "            c_non_temps,\n",
    "            [target_cols; c_input_temps],\n",
    "            [c_non_temps; c_input_temps],\n",
    "            target_cols,\n",
    "        )\n",
    "    end\n",
    "end;\n",
    "\n",
    "function create_TNNCell_from_data(tnsrs::TemperatureDataSet)\n",
    "    init_hidden = tnsrs.train_tnsr[2][1]\n",
    "    m = Flux.Recur(\n",
    "        TNNCell(\n",
    "            length(tnsrs.non_temp_cols) + length(tnsrs.inp_temp_cols),\n",
    "            length(tnsrs.temp_cols),\n",
    "            length(tnsrs.target_cols),\n",
    "            init_hidden,\n",
    "        ),\n",
    "        init_hidden,\n",
    "    )\n",
    "end;\n",
    "\n",
    "function get_data_tup(tnsrs::TemperatureDataSet)\n",
    "    return tnsrs.train_tnsr\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in DataFrame information\n",
    "test_p_idx = 0\n",
    "train_p_idx = 0\n",
    "@showprogress 0.5 \"Computing \" for (pid, df) in pairs(gdf)\n",
    "    if pid.profile_id ∈ test_set_pids\n",
    "        test_p_idx += 1\n",
    "        test_tensor_non_temp_x[1:nrow(df), :, test_p_idx] .= df[:, c_non_temps]\n",
    "        test_tensor_temps_x[1:nrow(df), :, test_p_idx] .= df[:, c_input_temps]\n",
    "        test_tensor_y[1:nrow(df), :, test_p_idx] .= df[:, target_cols]\n",
    "        test_sample_weights[1:nrow(df), test_p_idx] .= 1\n",
    "    else\n",
    "        train_p_idx += 1\n",
    "        train_tensor_non_temp_x[1:nrow(df), :, train_p_idx] .= df[:, c_non_temps]\n",
    "        train_tensor_temps_x[1:nrow(df), :, train_p_idx] .= df[:, c_input_temps]\n",
    "        train_tensor_y[1:nrow(df), :, train_p_idx] .= df[:, target_cols]\n",
    "        train_sample_weights[1:nrow(df), train_p_idx] .= 1\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbptt_len = 128\n",
    "\n",
    "train_vec_temps_x = [train_tensor_temps_x[i, :, :] for i in 1:size(train_tensor_temps_x, 1)]\n",
    "train_vec_non_temp_x = [train_tensor_non_temp_x[i, :, :] for i in 1:size(train_tensor_non_temp_x, 1)]\n",
    "train_vec_x = collect(zip(train_vec_non_temp_x, train_vec_temps_x))\n",
    "train_vec_y = [train_tensor_y[i, :, :] for i in 1:size(train_tensor_y, 1)]\n",
    "train_vec_sample_weights = [train_sample_weights[i, :] for i in 1:size(train_sample_weights, 1)]\n",
    "\n",
    "train_vec_chunked_x = []\n",
    "train_vec_chunked_y = []\n",
    "train_vec_chunked_w = []\n",
    "\n",
    "i = 0;\n",
    "while i*tbptt_len <= max_len_train\n",
    "    push!(train_vec_chunked_x, train_vec_x[i*tbptt_len+1:minimum(((i+1)*tbptt_len+1, max_len_train))])\n",
    "    push!(train_vec_chunked_y, train_vec_y[i*tbptt_len+1:minimum(((i+1)*tbptt_len+1, max_len_train))])\n",
    "    push!(train_vec_chunked_w, train_vec_sample_weights[i*tbptt_len+1:minimum(((i+1)*tbptt_len+1, max_len_train))])\n",
    "    global i+= 1\n",
    "end\n",
    "\n",
    "test_vec_temps_x = [test_tensor_temps_x[i, :, :] for i in 1:size(test_tensor_temps_x, 1)]\n",
    "test_vec_non_temp_x = [test_tensor_non_temp_x[i, :, :] for i in 1:size(test_tensor_non_temp_x, 1)]\n",
    "test_vec_x = collect(zip(train_vec_non_temp_x, test_vec_temps_x))\n",
    "test_vec_y = [test_tensor_y[i, :, :] for i in 1:size(test_tensor_y, 1)]\n",
    "test_vec_sample_weights = [test_sample_weights[i, :] for i in 1:size(test_sample_weights, 1)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1367008f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: train_vec_y not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: train_vec_y not defined\n",
      "\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "    @ ~/dev/projects/hyperdyn/examples/thermal_modeling/notebooks/prototyping.ipynb:4\n",
      "  [2] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      "  [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      "  [4] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      "  [5] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      "  [6] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      "  [7] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      "  [8] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      "  [9] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [10] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [11] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [12] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [13] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [14] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "using Statistics: mean\n",
    "n_epochs = 100\n",
    "pbar = Progress(n_epochs, desc=\"Training Epochs\", start=1, showspeed=true)\n",
    "init_hidden = train_vec_y[1]\n",
    "m = Flux.Recur(TNNCell(length(c_non_temps)+length(c_input_temps),\n",
    "                       length(c_temps),\n",
    "                       length(target_cols),\n",
    "                       init_hidden),\n",
    "               init_hidden)\n",
    "ps = params(m)\n",
    "opt = ADAM(1e-3)\n",
    "\n",
    "function sample_weighted_loss(x::Vector{Tuple{Matrix{T}, Matrix{T}}}, y::Vector{U}, w::Vector{V}) where {T, U, V}\n",
    "   mean(Flux.Losses.mse(m(xi), yi, agg=z->mean(wi' .* z)) for (xi, yi, wi) in zip(x, y, w)) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "data_tup = zip(train_vec_chunked_x, train_vec_chunked_y, train_vec_chunked_w);\n",
    "for epoch in 1:n_epochs\n",
    "    Flux.reset!(m)\n",
    "    Flux.train!(sample_weighted_loss, ps, data_tup, opt)\n",
    "    next!(pbar, showvalues = [(:epoch, epoch)])\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0103301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark Flux.train!($sample_weighted_loss, $ps, $data_tup, $opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3c4983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing   9%|███▋                                     |  ETA: 0:00:06\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  19%|███████▊                                 |  ETA: 0:00:05\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  28%|███████████▎                             |  ETA: 0:00:04\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  38%|███████████████▌                         |  ETA: 0:00:04\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  48%|███████████████████▋                     |  ETA: 0:00:03\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  65%|██████████████████████████▊              |  ETA: 0:00:02\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  78%|████████████████████████████████▏        |  ETA: 0:00:01\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing  88%|████████████████████████████████████▎    |  ETA: 0:00:01\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mComputing 100%|█████████████████████████████████████████| Time: 0:00:05\u001b[39m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "test_set_pids = [60, 62, 74]\n",
    "target_cols = [\"pm\", \"stator_tooth\", \"stator_winding\", \"stator_yoke\"]\n",
    "ds_df = load_dataset(\"/home/wilhelmk/dev/projects/datasets/kaggle_emotor_temps.csv\")\n",
    "ds = TemperatureDataSet(ds_df, \"profile_id\", test_set_pids, [0, 0], target_cols, 128)\n",
    "m = create_TNNCell_from_data(ds)\n",
    "ps = params(m);\n",
    "opt = ADAM(1e-3)\n",
    "\n",
    "function sample_weighted_loss(\n",
    "    x::Vector{Tuple{Matrix{T},Matrix{T}}},\n",
    "    y::Vector{U},\n",
    "    w::Vector{V},\n",
    ") where {T,U,V}\n",
    "    mean(\n",
    "        Flux.Losses.mse(m(xi), yi, agg = z -> mean(wi' .* z)) for\n",
    "        (xi, yi, wi) in zip(x, y, w)\n",
    "    )\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7da899e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: number of columns of each array must match (got (1, 66, 66))",
     "output_type": "error",
     "traceback": [
      "ArgumentError: number of columns of each array must match (got (1, 66, 66))\n",
      "\n",
      "Stacktrace:\n",
      "  [1] _typed_vcat(#unused#::Type{Any}, A::Tuple{Vector{Matrix{Float32}}, Matrix{Float32}, Matrix{Float32}})\n",
      "    @ Base ./abstractarray.jl:1553\n",
      "  [2] typed_vcat\n",
      "    @ ./abstractarray.jl:1567 [inlined]\n",
      "  [3] vcat\n",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/SparseArrays/src/sparsevector.jl:1114 [inlined]\n",
      "  [4] adjoint\n",
      "    @ ~/.julia/packages/Zygote/TaBlo/src/lib/array.jl:142 [inlined]\n",
      "  [5] _pullback\n",
      "    @ ~/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:65 [inlined]\n",
      "  [6] _pullback\n",
      "    @ ~/dev/projects/hyperdyn/examples/thermal_modeling/notebooks/prototyping.ipynb:78 [inlined]\n",
      "  [7] _pullback(::Zygote.Context, ::TNNCell{Chain{Tuple{Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}}}, Float32, Vector{Matrix{Float32}}}, ::Vector{Matrix{Float32}}, ::Tuple{Matrix{Float32}, Matrix{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface2.jl:0\n",
      "  [8] _pullback\n",
      "    @ ~/.julia/packages/Flux/Zz9RI/src/layers/recurrent.jl:34 [inlined]\n",
      "  [9] _pullback(ctx::Zygote.Context, f::Flux.Recur{TNNCell{Chain{Tuple{Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}, Dense{typeof(σ), Matrix{Float32}, Vector{Float32}}}}, Float32, Vector{Matrix{Float32}}}, Vector{Matrix{Float32}}}, args::Tuple{Matrix{Float32}, Matrix{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface2.jl:0\n",
      " [10] _pullback\n",
      "    @ ./none:0 [inlined]\n",
      " [11] _pullback(ctx::Zygote.Context, f::var\"#105#107\", args::Tuple{Tuple{Matrix{Float32}, Matrix{Float32}}, Matrix{Float32}, Vector{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface2.jl:0\n",
      " [12] _pullback\n",
      "    @ ./generator.jl:47 [inlined]\n",
      " [13] _pullback\n",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Statistics/src/Statistics.jl:62 [inlined]\n",
      " [14] _pullback(::Zygote.Context, ::typeof(mean), ::typeof(identity), ::Base.Generator{Base.Iterators.Zip{Tuple{Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, Vector{Matrix{Float32}}, Vector{Vector{Float32}}}}, var\"#105#107\"})\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface2.jl:0\n",
      " [15] _pullback\n",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Statistics/src/Statistics.jl:44 [inlined]\n",
      " [16] _pullback\n",
      "    @ ~/dev/projects/hyperdyn/examples/thermal_modeling/notebooks/prototyping.ipynb:14 [inlined]\n",
      " [17] _pullback(::Zygote.Context, ::typeof(sample_weighted_loss), ::Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, ::Vector{Matrix{Float32}}, ::Vector{Vector{Float32}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface2.jl:0\n",
      " [18] _apply(::Function, ::Vararg{Any, N} where N)\n",
      "    @ Core ./boot.jl:804\n",
      " [19] adjoint\n",
      "    @ ~/.julia/packages/Zygote/TaBlo/src/lib/lib.jl:200 [inlined]\n",
      " [20] _pullback(__context__::Zygote.Context, 454::typeof(Core._apply_iterate), 455::typeof(iterate), f::Function, args::Tuple{Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, Vector{Matrix{Float32}}, Vector{Vector{Float32}}})\n",
      "    @ Zygote ~/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:65\n",
      " [21] _pullback\n",
      "    @ ~/.julia/packages/Flux/Zz9RI/src/optimise/train.jl:105 [inlined]\n",
      " [22] _pullback(::Zygote.Context, ::Flux.Optimise.var\"#39#45\"{typeof(sample_weighted_loss), Tuple{Vector{Tuple{Matrix{Float32}, Matrix{Float32}}}, Vector{Matrix{Float32}}, Vector{Vector{Float32}}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface2.jl:0\n",
      " [23] pullback(f::Function, ps::Zygote.Params)\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface.jl:343\n",
      " [24] gradient(f::Function, args::Zygote.Params)\n",
      "    @ Zygote ~/.julia/packages/Zygote/TaBlo/src/compiler/interface.jl:75\n",
      " [25] macro expansion\n",
      "    @ ~/.julia/packages/Flux/Zz9RI/src/optimise/train.jl:104 [inlined]\n",
      " [26] macro expansion\n",
      "    @ ~/.julia/packages/Juno/n6wyj/src/progress.jl:134 [inlined]\n",
      " [27] train!(loss::Function, ps::Zygote.Params, data::Base.Iterators.Zip{Tuple{Vector{Any}, Vector{Any}, Vector{Any}}}, opt::ADAM; cb::Flux.Optimise.var\"#40#46\")\n",
      "    @ Flux.Optimise ~/.julia/packages/Flux/Zz9RI/src/optimise/train.jl:102\n",
      " [28] train!(loss::Function, ps::Zygote.Params, data::Base.Iterators.Zip{Tuple{Vector{Any}, Vector{Any}, Vector{Any}}}, opt::ADAM)\n",
      "    @ Flux.Optimise ~/.julia/packages/Flux/Zz9RI/src/optimise/train.jl:100\n",
      " [29] var\"##core#320\"(sample_weighted_loss#316::typeof(sample_weighted_loss), ps#317::Zygote.Params, zip#318::typeof(zip), opt#319::ADAM)\n",
      "    @ Main ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:489\n",
      " [30] var\"##sample#321\"(::Tuple{typeof(sample_weighted_loss), Zygote.Params, typeof(zip), ADAM}, __params::BenchmarkTools.Parameters)\n",
      "    @ Main ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:495\n",
      " [31] _run(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; verbose::Bool, pad::String, kwargs::Base.Iterators.Pairs{Symbol, Integer, NTuple{4, Symbol}, NamedTuple{(:samples, :evals, :gctrial, :gcsample), Tuple{Int64, Int64, Bool, Bool}}})\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:99\n",
      " [32] #invokelatest#2\n",
      "    @ ./essentials.jl:710 [inlined]\n",
      " [33] #run_result#45\n",
      "    @ ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:34 [inlined]\n",
      " [34] run(b::BenchmarkTools.Benchmark, p::BenchmarkTools.Parameters; progressid::Nothing, nleaves::Float64, ndone::Float64, kwargs::Base.Iterators.Pairs{Symbol, Integer, NTuple{5, Symbol}, NamedTuple{(:verbose, :samples, :evals, :gctrial, :gcsample), Tuple{Bool, Int64, Int64, Bool, Bool}}})\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:117\n",
      " [35] #warmup#54\n",
      "    @ ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:169 [inlined]\n",
      " [36] warmup(item::BenchmarkTools.Benchmark)\n",
      "    @ BenchmarkTools ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:169\n",
      " [37] top-level scope\n",
      "    @ ~/.julia/packages/BenchmarkTools/7xSXH/src/execution.jl:393\n",
      " [38] eval\n",
      "    @ ./boot.jl:360 [inlined]\n",
      " [39] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base ./loading.jl:1116\n",
      " [40] #invokelatest#2\n",
      "    @ ./essentials.jl:708 [inlined]\n",
      " [41] invokelatest\n",
      "    @ ./essentials.jl:706 [inlined]\n",
      " [42] (::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:18\n",
      " [43] withpath(f::VSCodeServer.var\"#146#147\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/repl.jl:185\n",
      " [44] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:14\n",
      " [45] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/JSONRPC/src/typed.jl:67\n",
      " [46] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/packages/VSCodeServer/src/serve_notebook.jl:94\n",
      " [47] top-level scope\n",
      "    @ ~/.vscode/extensions/julialang.language-julia-1.5.11/scripts/notebook/notebook.jl:12\n",
      " [48] include(mod::Module, _path::String)\n",
      "    @ Base ./Base.jl:386\n",
      " [49] exec_options(opts::Base.JLOptions)\n",
      "    @ Base ./client.jl:285\n",
      " [50] _start()\n",
      "    @ Base ./client.jl:485"
     ]
    }
   ],
   "source": [
    "@benchmark Flux.train!($sample_weighted_loss, $ps, $zip(ds.train_tnsr...), $opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_one_epoch()\n",
    "    Flux.train!($sample_weighted_loss, $ps, $data_tup, $opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark train_one_epoch()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
